## RESEARCH PAPERS 
### Theoretical:
- [Neural Networks are Decision Trees](https://arxiv.org/abs/2210.05189)
- [Cross-Validation Bias due to Unsupervised Preprocessing](https://academic.oup.com/jrsssb/article/84/4/1474/7073256?login=true)
- [The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)
- [LoRA: Low-Rank Adaptation of Large Language Models (included here as it has applications beyond LLMs)](https://arxiv.org/abs/2106.09685)
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)

### Image:
- ViT related:
    * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929)

    * [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

    * [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2)

    * [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

    * [A ConvNet for the 2020s (a CNN that implements several key components that contribute to the performance of Vision Transformers)](https://arxiv.org/abs/2201.03545)

    * [(CLIP) Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- Diffusion related:
    * [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

    * [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/abs/2006.11239)

    * [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)

- [Taming Transformers for High-Resolution Image Synthesis (VQGAN)](https://arxiv.org/abs/2012.09841)

- [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)

- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)

- [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037)

### NLP
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)

- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

- [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556)

- [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)

- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)

### 3D Rendering:
- [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)

- [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)

### Misc:
- [Human-level play in the game of Diplomacy by combining language models with strategic reasoning](https://www.science.org/doi/10.1126/science.ade9097)


# Contributing

Contributions are very welcome, please share back with the wider community (and get credited for it)!

Please have a look at the [CONTRIBUTING](contributing.md) guidelines, also have a read about our [licensing](https://github.com/Data-Science-Community-SRM/Resourceify/blob/master/LICENSE) policy.

---

Back to [main page (table of contents)](https://data-science-community-srm.github.io/Resourceify/)