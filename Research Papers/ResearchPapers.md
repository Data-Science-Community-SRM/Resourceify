## RESEARCH PAPERS 
### Theoretical:
- [Neural Networks are Decision Trees](https://arxiv.org/abs/2210.05189)
- [Cross-Validation Bias due to Unsupervised Preprocessing](https://academic.oup.com/jrsssb/article/84/4/1474/7073256?login=true)
- [The Forward-Forward Algorithm: Some Preliminary Investigations](https://arxiv.org/abs/2212.13345)
- [LoRA: Low-Rank Adaptation of Large Language Models (included here as it has applications beyond LLMs)](https://arxiv.org/abs/2106.09685)
- [Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets](https://arxiv.org/abs/2201.02177)
- [DoReMi: Optimizing Data Mixtures for Domain Adaptation](https://arxiv.org/abs/2305.10426): This paper introduces DoReMi, a method for optimizing data mixtures in domain adaptation, crucial for improving model generalization across different data distributions.
- [Scaling Laws for Reward Model Overoptimization](https://arxiv.org/abs/2401.00942): This paper explores the scaling laws related to reward model overoptimization, a critical issue in reinforcement learning from human feedback (RLHF) and related areas.

### Image:
- ViT related:
    * [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)](https://arxiv.org/abs/2010.11929)

    * [Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)

    * [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877v2)

    * [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

    * [A ConvNet for the 2020s (a CNN that implements several key components that contribute to the performance of Vision Transformers)](https://arxiv.org/abs/2201.03545)

    * [(CLIP) Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)
- Diffusion related:
    * [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)

    * [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/abs/2006.11239)

    * [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)

- [Taming Transformers for High-Resolution Image Synthesis (VQGAN)](https://arxiv.org/abs/2012.09841)

- [Segment Anything (SAM)](https://arxiv.org/abs/2304.02643)

- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/abs/2304.07193)

- [Bayesian Flow Networks](https://arxiv.org/abs/2308.07037)

### NLP
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)

- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)

- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

- [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556)

- [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/abs/2301.13688)

- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)

- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/abs/2302.04761)
- [Mixture-of-Experts Meets Instruction Tuning: Scaling to Extremely Large Language Models](https://arxiv.org/abs/2401.08296): This paper details the scaling of language models using a mixture-of-experts approach combined with instruction tuning, enabling the development of extremely large and capable models.
- [Long Context Language Modeling Beyond Position Interpolation](https://arxiv.org/abs/2401.04690): This paper explores techniques for extending the context window of language models beyond position interpolation, addressing a critical limitation in handling long-range dependencies.

### 3D Rendering:
- [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://arxiv.org/abs/2003.08934)

- [Highly accurate protein structure prediction with AlphaFold](https://www.nature.com/articles/s41586-021-03819-2)

### Misc:
- [Human-level play in the game of Diplomacy by combining language models with strategic reasoning](https://www.science.org/doi/10.1126/science.ade9097)
- **[DeepSeek-R1: Scaling Up Reasoning via Reinforcement Learning](https://arxiv.org/abs/2501.12948):** This paper introduces DeepSeek-R1-Zero and DeepSeek-R1, reasoning models developed using large-scale reinforcement learning. DeepSeek-R1 demonstrates comparable reasoning performance to OpenAI-o1-1217 and is open-sourced along with distilled dense models.

# Contributing

Contributions are very welcome, please share back with the wider community (and get credited for it)!

Please have a look at the [CONTRIBUTING](contributing.md) guidelines, also have a read about our [licensing](https://github.com/Data-Science-Community-SRM/Resourceify/blob/master/LICENSE) policy.

---

Back to [main page (table of contents)](https://data-science-community-srm.github.io/Resourceify/)